# custom_training_builder.py è©³ç´°è§£èª¬

## ğŸ“ ãƒ•ã‚¡ã‚¤ãƒ«æ¦‚è¦

ã“ã®ãƒ•ã‚¡ã‚¤ãƒ«ã¯ã€PyTorch Lightning ã‚’ä½¿ç”¨ã—ãŸå­¦ç¿’ã‚¨ãƒ³ã‚¸ãƒ³å…¨ä½“ã‚’æ§‹ç¯‰ã—ã¾ã™ã€‚

**ä¸»ãªè²¬å‹™ï¼š**
- ãƒ‡ãƒ¼ã‚¿ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ï¼ˆDataModuleï¼‰ã®æ§‹ç¯‰
- ãƒ¢ãƒ‡ãƒ«ãƒ»æå¤±é–¢æ•°ãƒ»è©•ä¾¡æŒ‡æ¨™ã®æ§‹ç¯‰
- ãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼ï¼ˆå­¦ç¿’ãƒ«ãƒ¼ãƒ—ç®¡ç†ï¼‰ã®æ§‹ç¯‰
- ãƒ­ã‚°è¨˜éŒ²ã‚·ã‚¹ãƒ†ãƒ ï¼ˆWandB/TensorBoardï¼‰ã®è¨­å®š

**æ¯”å–©:** ã€Œå·¥å ´ã®çµ„ç«‹ãƒ©ã‚¤ãƒ³ã®è¨­è¨ˆå›³ã€ã®ã‚ˆã†ãªã‚‚ã®

---

## ğŸ”§ ä¸»è¦ã‚¯ãƒ©ã‚¹ãƒ»é–¢æ•°

### 1. `TrainingEngine` ã‚¯ãƒ©ã‚¹ï¼ˆãƒ‡ãƒ¼ã‚¿ã‚¯ãƒ©ã‚¹ï¼‰

#### å½¹å‰²
å­¦ç¿’ã«å¿…è¦ãª3ã¤ã®ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã‚’1ã¤ã«ã¾ã¨ã‚ã‚‹å®¹å™¨ã€‚

#### å®šç¾©
```python
@dataclass(frozen=True)
class TrainingEngine:
    """Lightning training engine dataclass wrapping the lightning trainer, model and datamodule."""
    trainer: pl.Trainer              # å­¦ç¿’ãƒ«ãƒ¼ãƒ—ç®¡ç†
    model: pl.LightningModule        # ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆ
    datamodule: pl.LightningDataModule  # ãƒ‡ãƒ¼ã‚¿ç®¡ç†
```

#### ä½¿ç”¨ä¾‹
```python
# å­¦ç¿’ã‚¨ãƒ³ã‚¸ãƒ³ã®å–å¾—
engine = build_training_engine(cfg, worker)

# å„ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã«ã‚¢ã‚¯ã‚»ã‚¹
engine.trainer      # ãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼
engine.model        # ãƒ¢ãƒ‡ãƒ«
engine.datamodule   # ãƒ‡ãƒ¼ã‚¿ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«

# å­¦ç¿’å®Ÿè¡Œ
engine.trainer.fit(engine.model, engine.datamodule)
```

#### `frozen=True` ã®æ„å‘³
```python
# ä½œæˆå¾Œã¯å¤‰æ›´ä¸å¯ï¼ˆä¸å¤‰ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆï¼‰
engine.trainer = new_trainer  # âŒ ã‚¨ãƒ©ãƒ¼

# ã“ã‚Œã«ã‚ˆã‚Šã€æ„å›³ã—ãªã„å¤‰æ›´ã‚’é˜²ã
```

---

### 2. `update_config_for_training()` é–¢æ•°

#### å½¹å‰²
è¨­å®šã®å¦¥å½“æ€§ã‚’ãƒã‚§ãƒƒã‚¯ã—ã¦ã€å¿…è¦ã«å¿œã˜ã¦ä¿®æ­£ã™ã‚‹å‰å‡¦ç†ã€‚

#### ã‚·ã‚°ãƒãƒãƒ£
```python
def update_config_for_training(cfg: DictConfig) -> None:
```

#### å‡¦ç†å†…å®¹

```
å…¥åŠ›: cfgï¼ˆomegaconf è¨­å®šã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆï¼‰
 â†“
1. è¨­å®šã‚’ç·¨é›†å¯èƒ½ã«ã™ã‚‹
   OmegaConf.set_struct(cfg, False)
 â†“
2. ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ‘ã‚¹å‡¦ç†
   - None ãªã‚‰è­¦å‘Šãƒ­ã‚°
   - ãƒ­ãƒ¼ã‚«ãƒ«ãƒ‘ã‚¹ãªã‚‰è‡ªå‹•ä½œæˆ
   - S3ãƒ‘ã‚¹ãªã‚‰ä½•ã‚‚ã—ãªã„
 â†“
3. ã‚­ãƒ£ãƒƒã‚·ãƒ¥å‰Šé™¤ãŒæœ‰åŠ¹ãªã‚‰å®Ÿè¡Œ
   Path(cache_path).rmtree()  â† å¤ã„ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’æ¶ˆã™
 â†“
4. ã‚ªãƒ¼ãƒãƒ¼ãƒ•ã‚£ãƒƒãƒ†ã‚£ãƒ³ã‚°æ¤œæŸ»æœ‰åŠ¹ãªã‚‰
   num_workers = 0  â† ã‚·ãƒ³ã‚°ãƒ«ã‚¹ãƒ¬ãƒƒãƒ‰ã«å¤‰æ›´
 â†“
5. è¨­å®šã‚’èª­ã¿è¾¼ã¿å°‚ç”¨ã«
   OmegaConf.set_struct(cfg, True)
 â†“
6. æœ€çµ‚è¨­å®šã‚’ãƒ­ã‚°å‡ºåŠ›
```

#### å…·ä½“ä¾‹

```python
# ã€ã‚·ãƒŠãƒªã‚ª1ã€‘ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ‘ã‚¹ãŒ None
cfg.cache.cache_path = None
update_config_for_training(cfg)
# ãƒ­ã‚°: "Parameter cache_path is not set, caching is disabled"

# ã€ã‚·ãƒŠãƒªã‚ª2ã€‘ãƒ­ãƒ¼ã‚«ãƒ«ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ‘ã‚¹
cfg.cache.cache_path = "/tmp/cache"
cfg.cache.cleanup_cache = True
update_config_for_training(cfg)
# å‹•ä½œ: /tmp/cache ãŒå­˜åœ¨ã™ã‚Œã°å‰Šé™¤
#     æ–°è¦ã« /tmp/cache ã‚’ä½œæˆ

# ã€ã‚·ãƒŠãƒªã‚ª3ã€‘S3ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ‘ã‚¹
cfg.cache.cache_path = "s3://my-bucket/cache"
update_config_for_training(cfg)
# å‹•ä½œ: S3ãƒ‘ã‚¹ãªã®ã§ã‚¹ã‚­ãƒƒãƒ—ï¼ˆã‚¯ãƒ©ã‚¦ãƒ‰å´ã§ç®¡ç†ï¼‰
```

---

### 3. `build_lightning_datamodule()` é–¢æ•°

#### å½¹å‰²
PyTorch Lightning ã®ãƒ‡ãƒ¼ã‚¿ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’æ§‹ç¯‰ã™ã‚‹ã€‚

#### ã‚·ã‚°ãƒãƒãƒ£
```python
def build_lightning_datamodule(
    cfg: DictConfig,           # è¨­å®š
    worker: WorkerPool,        # ãƒãƒ«ãƒãƒ—ãƒ­ã‚»ãƒƒã‚·ãƒ³ã‚°
    model: TorchModuleWrapper  # ãƒ¢ãƒ‡ãƒ«ï¼ˆå¿…è¦ãªç‰¹å¾´é‡ã‚’å–å¾—ï¼‰
) -> pl.LightningDataModule:
```

#### å‡¦ç†ãƒ•ãƒ­ãƒ¼
```
å…¥åŠ›: cfg, worker, model
 â†“
1. ãƒ¢ãƒ‡ãƒ«ãŒå¿…è¦ã¨ã™ã‚‹ç‰¹å¾´é‡ã‚’å–å¾—
   feature_builders = model.get_list_of_required_feature()
 â†“
2. ãƒ¢ãƒ‡ãƒ«ãŒå‡ºåŠ›ã™ã‚‹ç›®æ¨™å€¤ã‚’å–å¾—
   target_builders = model.get_list_of_computed_target()
 â†“
3. train/val/teståˆ†å‰²å™¨ã‚’æ§‹ç¯‰
   splitter = build_splitter(cfg.splitter)
 â†“
4. ç‰¹å¾´é‡è¨ˆç®—ã‚¨ãƒ³ã‚¸ãƒ³ã‚’æ§‹ç¯‰
   feature_preprocessor = FeaturePreprocessor(...)
 â†“
5. ãƒ‡ãƒ¼ã‚¿æ‹¡å¼µï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰ã‚’æ§‹ç¯‰
   augmentors = build_agent_augmentor(cfg.data_augmentation)
 â†“
6. ã‚·ãƒŠãƒªã‚ªã‚’èª­ã¿è¾¼ã¿
   scenarios = build_scenarios(cfg, worker, model)
 â†“
7. ã‚«ã‚¹ã‚¿ãƒ ãƒ‡ãƒ¼ã‚¿ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’æ§‹ç¯‰
   datamodule = CustomDataModule(...)
 â†“
å‡ºåŠ›: datamodule
```

#### å…·ä½“çš„ãªå‡¦ç†

```python
# ã€ã‚­ãƒ¼å‡¦ç†ã€‘ãƒ¢ãƒ‡ãƒ«ä¾å­˜ã®è¨­å®š
feature_builders = model.get_list_of_required_feature()
# ä¾‹ãˆã° PLUTOãƒ¢ãƒ‡ãƒ«ãŒå¿…è¦ã¨ã™ã‚‹:
# - ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆä½ç½®æƒ…å ±
# - ãƒãƒƒãƒ—æƒ…å ±
# - éå»ã®è»Œè·¡æƒ…å ±
# ãªã©ã‚’è‡ªå‹•èªè­˜

# ã€ã‚­ãƒ¼å‡¦ç†ã€‘ç‰¹å¾´é‡è¨ˆç®—
feature_preprocessor = FeaturePreprocessor(
    cache_path=cfg.cache.cache_path,  # å‰å›ã®è¨ˆç®—çµæœã‚’å†åˆ©ç”¨
    force_feature_computation=cfg.cache.force_feature_computation,
    feature_builders=feature_builders,  # ãƒ¢ãƒ‡ãƒ«ãŒå¿…è¦ãªç‰¹å¾´é‡
    target_builders=target_builders      # å­¦ç¿’ã®ç›®æ¨™å€¤
)
```

---

### 4. `build_lightning_module()` é–¢æ•°

#### å½¹å‰²
PyTorch Lightning ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ï¼ˆãƒ¢ãƒ‡ãƒ« + æå¤±é–¢æ•° + æœ€é©åŒ–å™¨ï¼‰ã‚’æ§‹ç¯‰ã™ã‚‹ã€‚

#### ã‚·ã‚°ãƒãƒãƒ£
```python
def build_lightning_module(
    cfg: DictConfig,
    torch_module_wrapper: TorchModuleWrapper  # ãƒ™ãƒ¼ã‚¹ã¨ãªã‚‹ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆ
) -> pl.LightningModule:
```

#### å‡¦ç†ãƒ•ãƒ­ãƒ¼

```
å…¥åŠ›: cfg, torch_module_wrapper
 â†“
è¨­å®šã« `custom_trainer` ã‚»ã‚¯ã‚·ãƒ§ãƒ³ãŒã‚ã‚‹ã‹?
 â”‚
 â”œâ”€ YESï¼ˆã‚«ã‚¹ã‚¿ãƒ è¨“ç·´å™¨ãŒã‚ã‚‹ï¼‰
 â”‚   â†“
 â”‚  ã‚«ã‚¹ã‚¿ãƒ è¨“ç·´å™¨ã‚’ä½¿ç”¨
 â”‚   model = instantiate(
 â”‚       cfg.custom_trainer,
 â”‚       model=torch_module_wrapper,
 â”‚       lr=cfg.lr,
 â”‚       weight_decay=cfg.weight_decay,
 â”‚       epochs=cfg.epochs,
 â”‚       warmup_epochs=cfg.warmup_epochs
 â”‚   )
 â”‚
 â””â”€ NOï¼ˆæ¨™æº–è¨­å®šï¼‰
     â†“
    æ¨™æº– LightningModuleWrapper ã‚’ä½¿ç”¨
     - æå¤±é–¢æ•°ã‚’ build_objectives() ã§æ§‹ç¯‰
     - è©•ä¾¡æŒ‡æ¨™ã‚’ build_training_metrics() ã§æ§‹ç¯‰
     - æœ€é©åŒ–å™¨ã‚’ cfg.optimizer ã§æŒ‡å®š
     - å­¦ç¿’ç‡ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼ã‚’è¨­å®š
     â†“
å‡ºåŠ›: model
```

#### å…·ä½“ä¾‹

```python
# ã€ã‚·ãƒŠãƒªã‚ªAã€‘ã‚«ã‚¹ã‚¿ãƒ è¨“ç·´å™¨ä½¿ç”¨
# config.yaml ã«ä»¥ä¸‹ãŒã‚ã‚‹å ´åˆ:
# custom_trainer:
#   _target_: src.models.pluto.pluto_trainer.PlutoTrainer

model = PLUTOTrainer(
    model=pluto_model,
    lr=1e-3,
    weight_decay=0.0001,
    epochs=25,
    warmup_epochs=3
)

# ã€ã‚·ãƒŠãƒªã‚ªBã€‘æ¨™æº–è¨­å®š
# config.yaml ã« custom_trainer ãŒãªã„å ´åˆ:

objectives = build_objectives(cfg)  # [Loss1(), Loss2(), ...]
metrics = build_training_metrics(cfg)  # [Metric1(), Metric2(), ...]

model = LightningModuleWrapper(
    model=pluto_model,
    objectives=objectives,
    metrics=metrics,
    batch_size=32,
    optimizer=cfg.optimizer,
    lr_scheduler=cfg.lr_scheduler,
    warm_up_lr_scheduler=cfg.warm_up_lr_scheduler
)
```

---

### 5. `build_custom_trainer()` é–¢æ•°

#### å½¹å‰²
PyTorch Lightning ãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼ã‚’æ§‹ç¯‰ã€‚å­¦ç¿’ãƒ«ãƒ¼ãƒ—ã€ãƒ­ã‚®ãƒ³ã‚°ã€ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä¿å­˜ã‚’ç®¡ç†ã™ã‚‹ã€‚

#### ã‚·ã‚°ãƒãƒãƒ£
```python
def build_custom_trainer(cfg: DictConfig) -> pl.Trainer:
```

#### å‡¦ç†ãƒ•ãƒ­ãƒ¼

```
å…¥åŠ›: cfg
 â†“
1. ãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿å–å¾—
   params = cfg.lightning.trainer.params
   ä¾‹: max_epochs, gpus, num_nodes ãªã©
 â†“
2. ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯è¨­å®š
   â””â”€ ModelCheckpoint
      â””â”€ æœ€è‰¯ãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜
   â””â”€ RichModelSummary
      â””â”€ ãƒ¢ãƒ‡ãƒ«æ§‹é€ ã‚’è¡¨ç¤º
   â””â”€ RichProgressBar
      â””â”€ é€²æ—ãƒãƒ¼ã‚’è¡¨ç¤º
   â””â”€ LearningRateMonitor
      â””â”€ å­¦ç¿’ç‡ã®å¤‰åŒ–ã‚’è¨˜éŒ²
 â†“
3. ãƒ­ã‚®ãƒ³ã‚°è¨­å®š
   â”œâ”€ WandBæœ‰åŠ¹ â†’ WandbLogger
   â”‚  - WandB ã«ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ãƒ­ã‚°é€ä¿¡
   â”‚  - ã‚¢ãƒ¼ãƒ†ã‚£ãƒ•ã‚¡ã‚¯ãƒˆï¼ˆãƒ¢ãƒ‡ãƒ«ï¼‰ã‚’ä¿å­˜
   â”‚
   â””â”€ WandBç„¡åŠ¹ â†’ TensorBoardLogger
      - ãƒ­ãƒ¼ã‚«ãƒ«ã« TensorBoard ãƒ­ã‚°ä¿å­˜
 â†“
4. Trainer ç”Ÿæˆ
   trainer = pl.Trainer(
       callbacks=callbacks,
       logger=training_logger,
       **params
   )
 â†“
å‡ºåŠ›: trainer
```

#### WandB è¨­å®šã®è©³ç´°

```python
if cfg.wandb.mode == "disable":
    # WandB ã‚’ä½¿ã‚ãªã„
    training_logger = TensorBoardLogger(...)
else:
    # WandB ã‚’ä½¿ã†
    
    # ã€å‰å›ã®å®Ÿé¨“ã‚’ç¶šã‘ã‚‹å ´åˆã€‘
    if cfg.wandb.artifact is not None:
        # ã‚¢ãƒ¼ãƒ†ã‚£ãƒ•ã‚¡ã‚¯ãƒˆã‚’å–å¾—
        os.system(f"wandb artifact get {cfg.wandb.artifact}")
        
        # å‰å›ã®ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚’èª­ã¿è¾¼ã‚€
        checkpoint = os.path.join(os.getcwd(), f"artifacts/{artifact}/model.ckpt")
        run_id = artifact.split(":")[0][-8:]
        
        cfg.checkpoint = checkpoint
        cfg.wandb.run_id = run_id
    
    # WandbLogger ã‚’åˆæœŸåŒ–
    training_logger = WandbLogger(
        save_dir=cfg.group,
        project=cfg.wandb.project,      # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆå
        name=cfg.wandb.name,            # ãƒ©ãƒ³å
        mode=cfg.wandb.mode,            # "online" or "offline"
        log_model=cfg.wandb.log_model,  # "all", "best", or None
        resume=cfg.checkpoint is not None,  # å‰å›ã®ç¶šãã‹
        id=cfg.wandb.run_id             # ãƒ©ãƒ³ ID
    )
```

#### ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆè¨­å®šã®è©³ç´°

```python
ModelCheckpoint(
    dirpath=os.path.join(os.getcwd(), "checkpoints"),  # ä¿å­˜å…ˆ
    filename="{epoch}-{val_minFDE:.3f}",               # ãƒ•ã‚¡ã‚¤ãƒ«å
    monitor=cfg.lightning.trainer.checkpoint.monitor,  # ç›£è¦–å¯¾è±¡ï¼ˆä¾‹: val_lossï¼‰
    mode=cfg.lightning.trainer.checkpoint.mode,        # "min" or "max"
    save_top_k=cfg.lightning.trainer.checkpoint.save_top_k,  # ä¸Šä½Kå€‹ã‚’ä¿å­˜
    save_last=True                                      # æœ€å¾Œã®ã‚¨ãƒãƒƒã‚¯ã‚‚ä¿å­˜
)

# ä¾‹ãˆã° save_top_k=3 ãªã‚‰ã€æœ€è‰¯ã®3å€‹ã®ãƒ¢ãƒ‡ãƒ«ã®ã¿ä¿å­˜
# å¤ã„ãƒ¢ãƒ‡ãƒ«ã¯è‡ªå‹•å‰Šé™¤ã•ã‚Œã¦ãƒ‡ã‚£ã‚¹ã‚¯ç¯€ç´„
```

---

### 6. `build_training_engine()` é–¢æ•°ï¼ˆãƒ¡ã‚¤ãƒ³æ§‹ç¯‰é–¢æ•°ï¼‰

#### å½¹å‰²
å…¨ã¦ã®ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã‚’çµ±åˆã—ã¦ã€`TrainingEngine` ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’è¿”ã™ã€‚

#### ã‚·ã‚°ãƒãƒãƒ£
```python
def build_training_engine(
    cfg: DictConfig,
    worker: WorkerPool
) -> TrainingEngine:
```

#### å‡¦ç†ãƒ•ãƒ­ãƒ¼ï¼ˆå…¨ä½“çµ±åˆï¼‰

```
å…¥åŠ›: cfg, worker
 â†“
ã€ã‚¹ãƒ†ãƒƒãƒ—1ã€‘å‰å‡¦ç†
update_config_for_training(cfg)
 â”œâ”€ ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ‘ã‚¹ã®å¦¥å½“æ€§ãƒã‚§ãƒƒã‚¯
 â””â”€ è¨­å®šå€¤ã®è£œæ­£
 â†“
ã€ã‚¹ãƒ†ãƒƒãƒ—2ã€‘ãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼æ§‹ç¯‰
trainer = build_custom_trainer(cfg)
 â”œâ”€ ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯è¨­å®š
 â”œâ”€ ãƒ­ã‚°è¨˜éŒ²è¨­å®šï¼ˆWandB/TensorBoardï¼‰
 â””â”€ GPU/CPUè¨­å®š
 â†“
ã€ã‚¹ãƒ†ãƒƒãƒ—3ã€‘ãƒ¢ãƒ‡ãƒ«æ§‹ç¯‰
torch_module_wrapper = build_torch_module_wrapper(cfg.model)
 â””â”€ ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®åŸºæœ¬æ§‹é€ 
 â†“
ã€ã‚¹ãƒ†ãƒƒãƒ—4ã€‘ãƒ‡ãƒ¼ã‚¿ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«æ§‹ç¯‰
datamodule = build_lightning_datamodule(cfg, worker, torch_module_wrapper)
 â”œâ”€ ç‰¹å¾´é‡è¨ˆç®—ã‚¨ãƒ³ã‚¸ãƒ³
 â”œâ”€ train/val/teståˆ†å‰²
 â””â”€ DataLoader ç”Ÿæˆ
 â†“
ã€ã‚¹ãƒ†ãƒƒãƒ—5ã€‘ãƒ©ã‚¤ãƒˆãƒ‹ãƒ³ã‚°ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«æ§‹ç¯‰
model = build_lightning_module(cfg, torch_module_wrapper)
 â”œâ”€ æï¿½ï¿½ï¿½é–¢æ•°
 â”œâ”€ è©•ä¾¡æŒ‡æ¨™
 â””â”€ æœ€é©åŒ–å™¨
 â†“
ã€ã‚¹ãƒ†ãƒƒãƒ—6ã€‘çµ±åˆ
engine = TrainingEngine(
    trainer=trainer,
    model=model,
    datamodule=datamodule
)
 â†“
å‡ºåŠ›: engine
```

#### ä½¿ç”¨ä¾‹

```python
# run_training.py ãªã©ã‹ã‚‰å‘¼ã³å‡ºã—
engine = build_training_engine(cfg, worker)

# å­¦ç¿’å®Ÿè¡Œ
engine.trainer.fit(engine.model, engine.datamodule)

# ãƒ†ã‚¹ãƒˆå®Ÿè¡Œï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
engine.trainer.test(engine.model, engine.datamodule)
```

---

## ğŸ”„ å…¨ä½“çš„ãªãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ­ãƒ¼

```
ã€è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ã€‘
config/default_training.yaml
  â”œâ”€ epochs: 25
  â”œâ”€ lr: 1e-3
  â”œâ”€ batch_size: 32
  â”œâ”€ wandb.mode: online
  â””â”€ model, data_loader ç­‰
        â†“
ã€build_training_engine() å‘¼ã³å‡ºã—ã€‘
        â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  update_config_for_training()       â”‚
â”‚  â””â”€ ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ‘ã‚¹æº–å‚™ãªã©         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  build_custom_trainer()             â”‚
â”‚  â”œâ”€ callbacks: [checkpoint, ...]    â”‚
â”‚  â”œâ”€ logger: WandbLogger             â”‚
â”‚  â””â”€ trainer ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿çµ±åˆ          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  build_torch_module_wrapper()       â”‚
â”‚  â””â”€ ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆåŸºæœ¬æ§‹é€         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  build_lightning_datamodule()       â”‚
â”‚  â”œâ”€ feature_preprocessor           â”‚
â”‚  â”œâ”€ splitter                       â”‚
â”‚  â”œâ”€ scenarios                      â”‚
â”‚  â””â”€ CustomDataModule               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  build_lightning_module()           â”‚
â”‚  â”œâ”€ objectives (æå¤±é–¢æ•°)           â”‚
â”‚  â”œâ”€ metrics (è©•ä¾¡æŒ‡æ¨™)              â”‚
â”‚  â””â”€ optimizer                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  TrainingEngine çµ±åˆ                â”‚
â”‚  â”œâ”€ trainer                        â”‚
â”‚  â”œâ”€ model                          â”‚
â”‚  â””â”€ datamodule                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â†“
ã€å­¦ç¿’å®Ÿè¡Œã€‘
trainer.fit(model, datamodule)
  â”œâ”€ for epoch in range(max_epochs):
  â”‚   â”œâ”€ train loop
  â”‚   â”‚   â””â”€ å„ãƒãƒƒãƒã§é‡ã¿ã‚’æ›´æ–°
  â”‚   â”œâ”€ val loop
  â”‚   â”‚   â””â”€ æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ã§è©•ä¾¡
  â”‚   â””â”€ ãƒ­ã‚°è¨˜éŒ²ï¼ˆWandBï¼‰
  â””â”€ ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä¿å­˜
```

---

## ğŸ’¡ é‡è¦ãªãƒã‚¤ãƒ³ãƒˆ

### 1. è¨­å®šé§†å‹•è¨­è¨ˆï¼ˆConfiguration-driven Designï¼‰
```python
# ã‚³ãƒ¼ãƒ‰ã®å¤‰æ›´ãªã—ã«ã€è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã ã‘ã§å‹•ä½œå¤‰æ›´
- è¨“ç·´ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆå­¦ç¿’ç‡ã€ã‚¨ãƒãƒƒã‚¯æ•°ï¼‰
- ãƒ­ã‚®ãƒ³ã‚°è¨­å®šï¼ˆWandB or TensorBoardï¼‰
- ãƒ¢ãƒ‡ãƒ«ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£
- ãƒ‡ãƒ¼ã‚¿æ‹¡å¼µæ–¹æ³•
# ã™ã¹ã¦ YAML ã§ç®¡ç†
```

### 2. ãƒ“ãƒ«ãƒ€ãƒ¼ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆBuilder Patternï¼‰
```python
# è¤‡é›‘ãªã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã®ç”Ÿæˆã‚’æ®µéšçš„ã«è¡Œã†
build_custom_trainer()           # Step 1
build_torch_module_wrapper()     # Step 2
build_lightning_datamodule()     # Step 3
build_lightning_module()         # Step 4
TrainingEngine(...)              # Step 5 çµ±åˆ
```

### 3. ä¾å­˜æ€§æ³¨å…¥ï¼ˆDependency Injectionï¼‰
```python
# å„é–¢æ•°ãŒå¿…è¦ãªä¾å­˜æ€§ã‚’æ˜ç¤ºçš„ã«å—ã‘å–ã‚‹
def build_lightning_datamodule(cfg, worker, model):
    # cfg, worker, model ãŒã‚ã‚Œã°å‹•ä½œå¯èƒ½
    # ãƒ†ã‚¹ãƒˆæ™‚ã‚‚ç°¡å˜ã«ãƒ¢ãƒƒã‚¯å¯èƒ½
```

---

## ğŸ¯ å®Ÿè·µçš„ãªä½¿ç”¨ä¾‹

### ä¾‹1: WandB ã‚’ä½¿ã£ã¦å­¦ç¿’
```python
# run_training.py ãŒå†…éƒ¨ã§å®Ÿè¡Œ
cfg = load_config()
cfg.wandb.mode = "online"
cfg.wandb.project = "nuplan-pluto"
cfg.wandb.name = "experiment_1"

engine = build_training_engine(cfg, worker)
engine.trainer.fit(engine.model, engine.datamodule)

# WandB ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ã§ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ãƒ­ã‚°ã‚’ç¢ºèª
```

### ä¾‹2: å‰å›ã®å®Ÿé¨“ã‚’ç¶šã‘ã‚‹
```python
cfg = load_config()
cfg.wandb.mode = "online"
cfg.wandb.artifact = "my_project/my_run/model-v1:latest"

engine = build_training_engine(cfg, worker)
# å‰å›ã®ãƒ¢ãƒ‡ãƒ«ã‹ã‚‰è¨“ç·´å†é–‹
engine.trainer.fit(engine.model, engine.datamodule)
```

### ä¾‹3: TensorBoard ã§ãƒ­ã‚°è¨˜éŒ²
```python
cfg = load_config()
cfg.wandb.mode = "disable"  # WandB ç„¡åŠ¹

engine = build_training_engine(cfg, worker)
# TensorBoard ãƒ­ã‚°ãŒ ./logs ã«ä¿å­˜ã•ã‚Œã‚‹
engine.trainer.fit(engine.model, engine.datamodule)

# ãƒ–ãƒ©ã‚¦ã‚¶ã§ç¢ºèª:
# tensorboard --logdir logs
```

---

## ğŸ“š é–¢é€£ãƒ•ã‚¡ã‚¤ãƒ«

- [custom_datamodule.py](./custom_datamodule.md) - ãƒ‡ãƒ¼ã‚¿ç®¡ç†
- [../../../config/default_training.yaml](../../../config/default_training.yaml) - è¨­å®š
- [../../../run_training.py](../../../run_training.py) - ã‚¨ãƒ³ãƒˆãƒªãƒ¼ãƒã‚¤ãƒ³ãƒˆ
- [../../../README.md](../../../README.md) - ä½¿ç”¨æ–¹æ³•
