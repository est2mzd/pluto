# custom_training ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«è§£èª¬

## ğŸ“‹ æ¦‚è¦

`custom_training` ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã¯ã€PLUTOãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ï¼ˆãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ï¼‰ã‚’è¡Œã†ãŸã‚ã®ä¸­æ ¸æ©Ÿèƒ½ã‚’æä¾›ã—ã¾ã™ã€‚PyTorch Lightning ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã‚’ä½¿ç”¨ã—ã¦ã€ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã€ãƒ¢ãƒ‡ãƒ«æ§‹ç¯‰ã€å­¦ç¿’ãƒ«ãƒ¼ãƒ—ã®ç®¡ç†ã‚’è¡Œã„ã¾ã™ã€‚

### ğŸ¯ ä¸»ãªå½¹å‰²

```
å…¥åŠ›: è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆHydra Configï¼‰
        â†“
   [custom_training ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«]
        â†“
   - ãƒ‡ãƒ¼ã‚¿ã®æº–å‚™ã¨èª­ã¿è¾¼ã¿
   - ãƒ¢ãƒ‡ãƒ«ãƒ»æœ€é©åŒ–å™¨ã®æ§‹ç¯‰
   - å­¦ç¿’ãƒ«ãƒ¼ãƒ—ã®å®Ÿè¡Œ
   - ãƒ­ã‚°è¨˜éŒ²ï¼ˆWandB/TensorBoardï¼‰
        â†“
å‡ºåŠ›: å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ï¼ˆãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆï¼‰
```

---

## ğŸ“ ãƒ•ã‚¡ã‚¤ãƒ«æ§‹æˆ

| ãƒ•ã‚¡ã‚¤ãƒ« | å½¹å‰² | è©³ç´°ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ |
|---------|------|---------|
| `custom_datamodule.py` | ãƒ‡ãƒ¼ã‚¿ç®¡ç†ãƒ»ãƒ­ãƒ¼ãƒ‡ã‚£ãƒ³ã‚° | [è©³ç´°](./custom_datamodule.md) |
| `custom_training_builder.py` | å­¦ç¿’ã‚¨ãƒ³ã‚¸ãƒ³æ§‹ç¯‰ | [è©³ç´°](./custom_training_builder.md) |
| `__init__.py` | ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«å®šç¾© | å¤–éƒ¨å…¬é–‹ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ |

---

## ğŸ”„ ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ­ãƒ¼

### å­¦ç¿’ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®æµã‚Œ

```
1. ã€è¨­å®šèª­ã¿è¾¼ã¿ã€‘
   config/default_training.yaml â†’ Hydra Config
           â†“
2. ã€ãƒ‡ãƒ¼ã‚¿æº–å‚™ã€‘(custom_datamodule.py)
   - ã‚·ãƒŠãƒªã‚ªã®åˆ†å‰²ï¼ˆtrain/val/testï¼‰
   - ç‰¹å¾´é‡ã®è¨ˆç®—ã¨å‰å‡¦ç†
   - ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ä½œæˆ
           â†“
3. ã€ãƒ¢ãƒ‡ãƒ«æ§‹ç¯‰ã€‘(custom_training_builder.py)
   - ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ãƒ¢ãƒ‡ãƒ«
   - æå¤±é–¢æ•°ãƒ»è©•ä¾¡æŒ‡æ¨™
   - æœ€é©åŒ–å™¨ãƒ»å­¦ç¿’ç‡ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼
           â†“
4. ã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã€‘(PyTorch Lightning)
   - å„ã‚¨ãƒãƒƒã‚¯ã§å­¦ç¿’ãƒ«ãƒ¼ãƒ—å®Ÿè¡Œ
   - æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ã§è©•ä¾¡
   - ãƒ­ã‚°è¨˜éŒ²ï¼ˆWandBï¼‰
           â†“
5. ã€ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä¿å­˜ã€‘
   checkpoints/ â† æœ€è‰¯ãƒ¢ãƒ‡ãƒ«
```

---

## ğŸ”‘ ã‚­ãƒ¼ã‚³ãƒ³ã‚»ãƒ—ãƒˆ

### 1. **PyTorch Lightning ã‚’ä½¿ã†ç†ç”±**

é€šå¸¸ã®PyTorchã§ã¯ã€ä»¥ä¸‹ã‚’æ‰‹æ›¸ãã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ï¼š
```python
# é€šå¸¸ã®PyTorchï¼ˆè¤‡é›‘ï¼‰
for epoch in range(num_epochs):
    for batch in train_loader:
        # å‰å‡¦ç†ã€å­¦ç¿’ã‚¹ãƒ†ãƒƒãƒ—ã€æå¤±è¨ˆç®—...
        pass
    for val_batch in val_loader:
        # æ¤œè¨¼...
        pass
```

PyTorch Lightningã§ã¯ã€ç°¡æ½”ã«è¨˜è¿°ã§ãã¾ã™ï¼š
```python
# PyTorch Lightningï¼ˆã‚·ãƒ³ãƒ—ãƒ«ï¼‰
trainer = pl.Trainer(max_epochs=25)
trainer.fit(model, datamodule)
```

### 2. **Hydra Config ã‚·ã‚¹ãƒ†ãƒ **

è¨­å®šã‚’YAMLãƒ•ã‚¡ã‚¤ãƒ«ã§ç®¡ç†ã—ã€ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³ã‹ã‚‰å‹•çš„ã«å¤‰æ›´ã§ãã¾ã™ï¼š
```bash
python run_training.py \
  py_func=train \
  lr=1e-3 \
  batch_size=32 \
  wandb.mode=online
```

### 3. **ãƒ‡ãƒ¼ã‚¿åˆ†å‰²æˆ¦ç•¥**

- **Train Set**: ãƒ¢ãƒ‡ãƒ«ã®é‡ã¿ã‚’æ›´æ–°ã™ã‚‹ãƒ‡ãƒ¼ã‚¿
- **Val Set**: å­¦ç¿’ä¸­ã«ãƒ¢ãƒ‡ãƒ«æ€§èƒ½ã‚’è©•ä¾¡ã™ã‚‹ãƒ‡ãƒ¼ã‚¿  
- **Test Set**: æœ€çµ‚çš„ãªæ€§èƒ½è©•ä¾¡ç”¨ãƒ‡ãƒ¼ã‚¿ï¼ˆå­¦ç¿’ã«ã¯ä½¿ã‚ãªã„ï¼‰

---

## ğŸ“Š ä¸»è¦ã‚¯ãƒ©ã‚¹ãƒ»é–¢æ•°

### custom_datamodule.py

| åå‰ | ç¨®é¡ | èª¬æ˜ |
|------|------|------|
| `CustomDataModule` | ã‚¯ãƒ©ã‚¹ | PyTorch Lightningå¯¾å¿œã®ãƒ‡ãƒ¼ã‚¿ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ« |
| `create_dataset()` | é–¢æ•° | ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆ |
| `distributed_weighted_sampler_init()` | é–¢æ•° | ã‚·ãƒŠãƒªã‚ªã‚¿ã‚¤ãƒ—åˆ¥ã®é‡ã¿ä»˜ãã‚µãƒ³ãƒ—ãƒªãƒ³ã‚° |

### custom_training_builder.py

| åå‰ | ç¨®é¡ | èª¬æ˜ |
|------|------|------|
| `TrainingEngine` | ã‚¯ãƒ©ã‚¹ | trainer, model, datamoduleã‚’çµ±åˆ |
| `build_lightning_datamodule()` | é–¢æ•° | DataModuleã®æ§‹ç¯‰ |
| `build_lightning_module()` | é–¢æ•° | ãƒ¢ãƒ‡ãƒ«ãƒ»æå¤±é–¢æ•°ã®æ§‹ç¯‰ |
| `build_custom_trainer()` | é–¢æ•° | Trainerã®æ§‹ç¯‰ï¼ˆãƒ­ã‚°è¨˜éŒ²ã‚’å«ã‚€ï¼‰ |
| `build_training_engine()` | é–¢æ•° | å…¨ä½“çµ±åˆï¼ˆãƒ¡ã‚¤ãƒ³æ§‹ç¯‰é–¢æ•°ï¼‰ |
| `update_config_for_training()` | é–¢æ•° | è¨­å®šã®å¦¥å½“æ€§ãƒã‚§ãƒƒã‚¯ |

---

## ğŸš€ ä½¿ç”¨ä¾‹

### åŸºæœ¬çš„ãªå­¦ç¿’å®Ÿè¡Œ

```bash
cd /home/takuya/work/autonomous/pluto

# æœ€å°é™ã®è¨­å®šã§å­¦ç¿’
python run_training.py \
  py_func=train \
  +training=train_pluto \
  scenario_builder=nuplan \
  cache.cache_path=/path/to/cache \
  cache.use_cache_without_dataset=true
```

#### å¼•æ•°ã®èª¬æ˜

| å¼•æ•° | èª¬æ˜ | ä¾‹ |
|------|------|-----|
| `py_func=train` | å®Ÿè¡Œãƒ¢ãƒ¼ãƒ‰ ã‚’ã€Œè¨“ç·´ã€ã«æŒ‡å®š | `train` / `cache` / `test` |
| `+training=train_pluto` | PLUTOãƒ¢ãƒ‡ãƒ«ç”¨è¨“ç·´è¨­å®šã‚’èª­ã¿è¾¼ã¿ï¼ˆãƒ•ã‚¡ã‚¤ãƒ«: `config/training/train_pluto.yaml`ï¼‰ | `train_pluto` |
| `scenario_builder=nuplan` | nuPlanãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä½¿ç”¨ | `nuplan` |
| `cache.cache_path=/path/to/cache` | ç‰¹å¾´é‡ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã®ä¿å­˜å…ˆ | `/nuplan/exp/cache` |
| `cache.use_cache_without_dataset=true` | ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãŒã‚ã‚Œã°ã€å…ƒã® .db ãƒ•ã‚¡ã‚¤ãƒ«ãªã—ã§å­¦ç¿’ | `true` / `false` |

**è©³ç´°èª¬æ˜ï¼š**
- **`py_func=train`**: ã‚¹ã‚¯ãƒªãƒ—ãƒˆãŒã€Œå­¦ç¿’ã€ã€Œã‚­ãƒ£ãƒƒã‚·ãƒ¥ç”Ÿæˆã€ã€Œãƒ†ã‚¹ãƒˆã€ã®ã©ã‚Œã‚’å®Ÿè¡Œã™ã‚‹ã‹æŒ‡å®š
  - `train`: ãƒ¢ãƒ‡ãƒ«ã®é‡ã¿ã‚’æ›´æ–°
  - `cache`: ç‰¹å¾´é‡ã‚’è¨ˆç®—ãƒ»ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜ï¼ˆ`use_cache_without_dataset=true` ã®å‰ã«å®Ÿè¡ŒãŒå¿…è¦ï¼‰
  - `test`: å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã§æ€§èƒ½ã‚’è©•ä¾¡

- **`+training=train_pluto`**: `+` ã¯æ–°ã—ã„ã‚­ãƒ¼è¿½åŠ ã‚’æ„å‘³ã™ã‚‹ã€‚PLUTOç”¨ã®è¨“ç·´è¨­å®šï¼ˆæå¤±é–¢æ•°ã€ãƒ¡ãƒˆãƒªã‚¯ã‚¹ãªã©ï¼‰ã‚’èª­ã¿è¾¼ã‚€

- **`cache.cache_path`**: ä»¥ä¸‹ã‚’å«ã‚€ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
  - `/path/to/cache/train/`: è¨“ç·´ç”¨ç‰¹å¾´é‡
  - `/path/to/cache/val/`: æ¤œè¨¼ç”¨ç‰¹å¾´é‡
  - `/path/to/cache/test/`: ãƒ†ã‚¹ãƒˆç”¨ç‰¹å¾´é‡

- **`cache.use_cache_without_dataset=true`**: ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰ç›´æ¥èª­ã¿è¾¼ã‚€ã€‚å…ƒã® nuPlan .db ãƒ•ã‚¡ã‚¤ãƒ«ãŒä¸è¦ã«ãªã‚‹ã®ã§ã€ãƒ‡ãƒã‚¤ã‚¹ã®ç©ºãå®¹é‡ç¯€ç´„

---

### WandB ã‚’æœ‰åŠ¹ã«ã—ã¦å®Ÿè¡Œ

```bash
wandb login  # åˆå›ã®ã¿ã€APIã‚­ãƒ¼ã‚’å…¥åŠ›ï¼ˆhttps://wandb.ai/authorize ã‹ã‚‰å–å¾—ï¼‰

python run_training.py \
  py_func=train \
  +training=train_pluto \
  scenario_builder=nuplan \
  cache.cache_path=/path/to/cache \
  cache.use_cache_without_dataset=true \
  wandb.mode=online \
  wandb.project=nuplan-pluto \
  wandb.name=my_experiment
```

#### WandB å¼•æ•°ã®èª¬æ˜

| å¼•æ•° | èª¬æ˜ | ä¾‹ |
|------|------|-----|
| `wandb.mode=online` | WandBã‚’æœ‰åŠ¹åŒ–ï¼ˆ`disable` ã§ç„¡åŠ¹ï¼‰ | `online` / `offline` / `disable` |
| `wandb.project=nuplan-pluto` | WandBãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆå | ä»»æ„ï¼ˆãªã„ãªã‚‰è‡ªå‹•ä½œæˆï¼‰ |
| `wandb.name=my_experiment` | ãƒ©ãƒ³ã®è¡¨ç¤ºåï¼ˆWandBãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ã§è¦‹ãˆã‚‹ï¼‰ | `exp_1`, `baseline`, ãªã© |

**WandB ã®å‹•ä½œï¼š**
1. `wandb login` ã§ã‚¢ã‚«ã‚¦ãƒ³ãƒˆèªè¨¼
2. `mode=online` ã§ã€è¨“ç·´ä¸­ã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã§WandBã‚µãƒ¼ãƒãƒ¼ã«é€ä¿¡
3. ãƒ–ãƒ©ã‚¦ã‚¶ã§ https://wandb.ai ã«ã‚¢ã‚¯ã‚»ã‚¹ â†’ ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã‚’ç¢ºèª â†’ ã‚°ãƒ©ãƒ•ã‚’å¯è¦–åŒ–

**`mode=offline` ã¨ã®é•ã„ï¼š**
- `online`: ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ãƒ­ã‚°ï¼ˆã‚¤ãƒ³ã‚¿ãƒ¼ãƒãƒƒãƒˆæ¥ç¶šãŒå¿…è¦ï¼‰
- `offline`: ãƒ­ãƒ¼ã‚«ãƒ«ä¿å­˜ã®ã¿ï¼ˆå¾Œã§ `wandb sync` ã§åŒæœŸå¯èƒ½ï¼‰
- `disable`: WandBä½¿ã‚ãªã„ï¼ˆTensorBoardä½¿ç”¨ï¼‰

---

### è¨­å®šãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚º

```bash
python run_training.py \
  py_func=train \
  +training=train_pluto \
  scenario_builder=nuplan \
  cache.cache_path=/path/to/cache \
  cache.use_cache_without_dataset=true \
  lr=5e-4 \
  epochs=50 \
  warmup_epochs=5 \
  data_loader.params.batch_size=64 \
  data_loader.params.num_workers=8
```

#### è¨“ç·´ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®èª¬æ˜

| å¼•æ•° | èª¬æ˜ | ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ | èª¿æ•´ã®ãƒ’ãƒ³ãƒˆ |
|------|------|----------|-----------|
| `lr` | å­¦ç¿’ç‡ï¼ˆå¤§ãã„ã¨å­¦ç¿’ãŒæ—©ã„ãŒä¸å®‰å®šã€å°ã•ã„ã¨é…ã„ï¼‰ | `1e-3` | GPU VRAM ä¸è¶³ãªã‚‰å°ã•ãã™ã‚‹ |
| `epochs` | è¨“ç·´ã‚¨ãƒãƒƒã‚¯æ•° | `25` | å¤šã„ã»ã©ç²¾åº¦å‘ä¸Šï¼ˆè¨ˆç®—æ™‚é–“å¢—ï¼‰ |
| `warmup_epochs` | ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—ã‚¨ãƒãƒƒã‚¯æ•°ï¼ˆæœ€åˆã¯å­¦ç¿’ç‡ã‚’ä½ãã™ã‚‹ï¼‰ | `3` | ä¸å®‰å®šã•ã‚’è»½æ¸› |
| `data_loader.params.batch_size` | ãƒãƒƒãƒã‚µã‚¤ã‚ºï¼ˆå¤§ãã„ã¨å­¦ç¿’ãŒå®‰å®šï¼‰ | `32` | GPU ãƒ¡ãƒ¢ãƒªã«å¿œã˜ã¦èª¿æ•´ |
| `data_loader.params.num_workers` | ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã®ä¸¦åˆ—ãƒ¯ãƒ¼ã‚«ãƒ¼æ•° | è¨­å®šä¾å­˜ | CPU ã‚³ã‚¢æ•°ã«å¿œã˜ã¦èª¿æ•´ |

**å…·ä½“çš„ãªèª¿æ•´ä¾‹ï¼š**

```bash
# GPU ãƒ¡ãƒ¢ãƒªãŒé™ã‚‰ã‚Œã¦ã„ã‚‹å ´åˆ
python run_training.py ... \
  data_loader.params.batch_size=16 \
  data_loader.params.num_workers=4

# é«˜é€Ÿã«è¨“ç·´ã—ãŸã„å ´åˆ
python run_training.py ... \
  lr=2e-3 \
  epochs=100 \
  data_loader.params.batch_size=128 \
  data_loader.params.num_workers=16

# ç²¾å¯†ã«è¨“ç·´ã—ãŸã„å ´åˆï¼ˆæ™‚é–“ãŒã‹ã‹ã‚‹ï¼‰
python run_training.py ... \
  lr=1e-4 \
  epochs=200 \
  warmup_epochs=10 \
  data_loader.params.batch_size=16
```

---

### ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‹ã‚‰å†é–‹

```bash
# å‰å›ã®æœ€è‰¯ãƒ¢ãƒ‡ãƒ«ã‹ã‚‰è¨“ç·´ã‚’ç¶šã‘ã‚‹
python run_training.py \
  py_func=train \
  +training=train_pluto \
  scenario_builder=nuplan \
  cache.cache_path=/path/to/cache \
  cache.use_cache_without_dataset=true \
  checkpoint=./checkpoints/epoch-24-val_minFDE=0.123.ckpt
```

#### ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆå¼•æ•°

| å¼•æ•° | èª¬æ˜ |
|------|------|
| `checkpoint=./checkpoints/...ckpt` | ä¿å­˜ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ |

**ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä½ç½®:**
```
./checkpoints/
â”œâ”€â”€ epoch-20-val_minFDE=0.145.ckpt  â† 20ã‚¨ãƒãƒƒã‚¯ç›®
â”œâ”€â”€ epoch-24-val_minFDE=0.123.ckpt  â† 24ã‚¨ãƒãƒƒã‚¯ç›®ï¼ˆæœ€è‰¯ï¼‰
â””â”€â”€ last.ckpt                        â† æœ€å¾Œã®ã‚¨ãƒãƒƒã‚¯
```

---

## âš™ï¸ è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«

### ä¸»è¦ãªè¨­å®šé …ç›®ï¼ˆYAMLï¼‰

[default_training.yaml](../../../config/default_training.yaml) ã«è¨˜è¿°ã•ã‚Œã‚‹ã€ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®šï¼š

| é …ç›® | ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ | èª¬æ˜ |
|------|----------|------|
| `epochs` | `25` | å­¦ç¿’ã‚¨ãƒãƒƒã‚¯æ•°ï¼ˆç¹°ã‚Šè¿”ã—å›æ•°ï¼‰ |
| `warmup_epochs` | `3` | ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—ã‚¨ãƒãƒƒã‚¯ï¼ˆæœ€åˆã®æ•°ã‚¨ãƒãƒƒã‚¯ï¼‰ |
| `lr` | `1e-3` | å­¦ç¿’ç‡ï¼ˆé‡ã¿ã®æ›´æ–°é‡ï¼‰ |
| `weight_decay` | `0.0001` | L2æ­£å‰‡åŒ–ï¼ˆéå­¦ç¿’ã‚’æŠ‘ãˆã‚‹ï¼‰ |
| `data_loader.params.batch_size` | `32` | ä¸€åº¦ã«å‡¦ç†ã™ã‚‹ã‚µãƒ³ãƒ—ãƒ«æ•° |
| `data_loader.params.num_workers` | è¨­å®šä¾å­˜ | ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã®ä¸¦åˆ—æ•° |
| `cache.cache_path` | æœªè¨­å®š | ç‰¹å¾´é‡ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã®ä¿å­˜å…ˆ |
| `cache.use_cache_without_dataset` | `false` | ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã®ã¿ã§è¨“ç·´ã™ã‚‹ã‹ |
| `wandb.mode` | `disable` | `online` (æœ‰åŠ¹) / `disable` (ç„¡åŠ¹) |
| `wandb.project` | `nuplan-pluto` | WandB ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆå |

### è¨­å®šã®å„ªå…ˆé †ä½

```
ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³å¼•æ•°  > config/default_training.yaml > å„ç¨®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤

ä¾‹:
python run_training.py lr=5e-4           # â† ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³å¼•æ•°ãŒå„ªå…ˆ
# (default_training.yaml ã® lr=1e-3 ã‚’ä¸Šæ›¸ã)
```

### ã‚ˆãä½¿ã†è¨­å®šçµ„ã¿åˆã‚ã›

#### ãƒ‘ã‚¿ãƒ¼ãƒ³1: æœ€å°é™ã®è¨­å®š
```yaml
# default_training.yaml çš„ãªæœ€å°è¨­å®š
py_func: train
epochs: 25
lr: 1e-3
batch_size: 32
wandb:
  mode: disable
```

#### ãƒ‘ã‚¿ãƒ¼ãƒ³2: æœ¬æ ¼çš„ãªè¨“ç·´
```yaml
epochs: 50
lr: 5e-4
warmup_epochs: 5
weight_decay: 0.0001
batch_size: 64
num_workers: 16
wandb:
  mode: online
  project: nuplan-pluto
  name: production_v1
```

#### ãƒ‘ã‚¿ãƒ¼ãƒ³3: é«˜é€Ÿãªå®Ÿé¨“ç”¨
```yaml
epochs: 10
lr: 1e-3
batch_size: 128
num_workers: 8
wandb:
  mode: offline  # ãƒ­ãƒ¼ã‚«ãƒ«ã§ã®ã¿ä¿å­˜
```

---

## ğŸ”§ å¼•æ•°ã®è¦‹æ–¹ãƒ»èª­ã¿æ–¹

### ãƒ‰ãƒƒãƒˆè¨˜æ³•ï¼ˆdot notationï¼‰

```bash
# âŒ é–“é•ã„
python run_training.py batch_size=32

# âœ… æ­£ã—
python run_training.py data_loader.params.batch_size=32
```

**ç†ç”±ï¼š** YAML è¨­å®šãŒéšå±¤æ§‹é€ ã«ãªã£ã¦ã„ã‚‹ãŸã‚

```yaml
# config.yaml ã®ãƒ•ã‚¡ã‚¤ãƒ«æ§‹é€ 
data_loader:        # ãƒ¬ãƒ™ãƒ«1
  params:           # ãƒ¬ãƒ™ãƒ«2
    batch_size: 32  # ãƒ¬ãƒ™ãƒ«3
```

### `+` ã¨ `~` è¨˜å·

| è¨˜å· | æ„å‘³ | ä¾‹ |
|------|------|-----|
| `+key=value` | ã‚­ãƒ¼ã‚’è¿½åŠ ï¼ˆå­˜åœ¨ã—ãªã‹ã£ãŸã‚‰è¿½åŠ ï¼‰ | `+training=train_pluto` |
| `~key` | ã‚­ãƒ¼ã‚’å‰Šé™¤ | `~wandb.artifact` |
| `key=value` | ã‚­ãƒ¼ã‚’ä¸Šæ›¸ã | `lr=1e-3` |

### å‹ã®æŒ‡å®š

```bash
# æ–‡å­—åˆ—
python run_training.py wandb.project=my_project

# æ•°å€¤
python run_training.py lr=0.001

# çœŸå½å€¤
python run_training.py cache.use_cache_without_dataset=true

# ãƒªã‚¹ãƒˆ
python run_training.py 'worker=single_machine_thread_pool'
```

---

## ğŸ” ãƒ‡ãƒãƒƒã‚°ã®ãƒ’ãƒ³ãƒˆ

### å•é¡Œ: ãƒ¡ãƒ¢ãƒªä¸è¶³
**åŸå› **: ãƒãƒƒãƒã‚µã‚¤ã‚ºãŒå¤§ãã™ãã‚‹  
**è§£æ±º**: ä»¥ä¸‹ã‚’æ¸›ã‚‰ã™
```bash
data_loader.params.batch_size=32  # å…ƒ: 64
```

### å•é¡Œ: ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ãŒé…ã„
**åŸå› **: ãƒ¯ãƒ¼ã‚«ãƒ¼æ•°ãŒä¸é©åˆ‡  
**è§£æ±º**: GPUæ•°ã«å¿œã˜ã¦èª¿æ•´
```bash
data_loader.params.num_workers=4  # å…ƒ: 16
```

### å•é¡Œ: WandB ã¸ã®ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ãŒé…ã„
**åŸå› **: ãƒ­ã‚°é »åº¦ãŒé«˜ã™ãã‚‹  
**è§£æ±º**: ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä¿å­˜é »åº¦ã‚’èª¿æ•´
```yaml
lightning:
  trainer:
    checkpoint:
      save_top_k: 3  # æœ€è‰¯ãƒ¢ãƒ‡ãƒ«3ã¤ã®ã¿ä¿å­˜
```

---

## ğŸ“š å‚è€ƒè³‡æ–™

- [PyTorch Lightning å…¬å¼ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ](https://lightning.ai/)
- [Hydra å…¬å¼ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ](https://hydra.cc/)
- [WandB å…¬å¼ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ](https://docs.wandb.ai/)

---

## æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—

è©³ç´°ã¯å„ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’å‚ç…§ã—ã¦ãã ã•ã„ï¼š

- ğŸ“„ [custom_datamodule.py ã®è©³ç´°](./custom_datamodule.md)
- ğŸ“„ [custom_training_builder.py ã®è©³ç´°](./custom_training_builder.md)
